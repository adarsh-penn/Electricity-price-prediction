{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c88abed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyWavelets\n",
      "  Downloading PyWavelets-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /home/dxr5576/anaconda3/envs/TRA1/lib/python3.7/site-packages (from PyWavelets) (1.21.6)\n",
      "Installing collected packages: PyWavelets\n",
      "Successfully installed PyWavelets-1.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b67262",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from keras import Sequential\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Bidirectional, BatchNormalization, Dropout, Dense, Flatten, Conv1D\n",
    "from keras.layers import MaxPooling1D, GRU, Input,Masking, Concatenate, dot\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.losses import MeanAbsoluteError\n",
    "from keras.metrics import RootMeanSquaredError\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.regularizers import l1, l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from tensorflow.keras.optimizers import legacy\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9bd1b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70128, 76)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('UpdatedDataSet.xlsx')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b766eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac61eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pauta_criterion(series, threshold=3):\n",
    "    \"\"\"\n",
    "    Identifies outliers in a series using the Pauta criterion (3-sigma rule).\n",
    "    \n",
    "    Parameters:\n",
    "    - series: Pandas Series to analyze for outliers.\n",
    "    - threshold: The number of standard deviations to use as the cutoff for outliers.\n",
    "    \n",
    "    Returns:\n",
    "    - A boolean mask indicating which data points are outliers.\n",
    "    \"\"\"\n",
    "    mean = series.mean()\n",
    "    std_dev = series.std()\n",
    "    outliers = (series - mean).abs() > threshold * std_dev\n",
    "    return ~outliers  # Invert mask to keep non-outliers\n",
    "\n",
    "# Filter data to exclude outliers based on Pauta criterion\n",
    "non_outliers_mask = pauta_criterion(df['Target'])  # Apply only to 'Target' column\n",
    "df_filtered = df[non_outliers_mask]\n",
    "\n",
    "# Apply Min-Max scaling to all columns after removing outliers\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df_filtered), columns=df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13cc026c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Features based on Information Gain (IG):\n",
      "Index(['Feat 19', 'Feat 15', 'Feat 18', 'Feat 13', 'Feat 5', 'Feat 11',\n",
      "       'Feat 8', 'Feat 4', 'Feat 2', 'Feat 48', 'Feat 7', 'Feat 14', 'Feat 17',\n",
      "       'Feat 12', 'Feat 10', 'Feat 3', 'Feat 6', 'Feat 9', 'Feat 1',\n",
      "       'Feat 16'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "X_full = df_scaled.drop(columns=['Target'])  # All features except 'Target'\n",
    "y = df_scaled['Target']  # Target variable\n",
    "\n",
    "# Step 2: Calculate Information Gain (IG) and select top 20 features\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import numpy as np\n",
    "\n",
    "# Calculate Information Gain scores\n",
    "ig_scores = mutual_info_regression(X_full, y)\n",
    "\n",
    "# Get indices of the top 20 features\n",
    "ig_top20_indices = np.argsort(ig_scores)[-20:]  # Sort and select top 20 feature indices\n",
    "\n",
    "# Select the top 20 features based on IG scores\n",
    "X = X_full.iloc[:, ig_top20_indices]\n",
    "\n",
    "# Display the selected features\n",
    "print(\"\\nTop 20 Features based on Information Gain (IG):\")\n",
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "710a8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming `X` is the DataFrame with the top 20 selected features from IG\n",
    "# and `y` is the target variable\n",
    "\n",
    "# Define a function to apply Wavelet Transformation to each feature\n",
    "def wavelet_transform(X, wavelet='db1', level=1):\n",
    "    \"\"\"\n",
    "    Apply wavelet transform to each feature in X.\n",
    "\n",
    "    Parameters:\n",
    "    - X: DataFrame of input features.\n",
    "    - wavelet: Type of wavelet to use for the transform (default: 'db1').\n",
    "    - level: Decomposition level (default: 1).\n",
    "\n",
    "    Returns:\n",
    "    - Transformed DataFrame where each feature has been wavelet-transformed.\n",
    "    \"\"\"\n",
    "    transformed_data = []\n",
    "\n",
    "    # Apply wavelet transform to each feature column\n",
    "    for column in X.columns:\n",
    "        coeffs = pywt.wavedec(X[column], wavelet, level=level)\n",
    "        transformed_column = np.hstack(coeffs)  # Combine coefficients into a single array\n",
    "        transformed_data.append(transformed_column[:len(X)])  # Match original length\n",
    "\n",
    "    return pd.DataFrame(np.array(transformed_data).T, columns=X.columns)\n",
    "\n",
    "# Apply Wavelet Transformation\n",
    "X_wavelet = wavelet_transform(X)\n",
    "\n",
    "# Split the transformed data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_wavelet, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Reshape data for LSTM (samples, timesteps, features)\n",
    "X_train_reshaped = np.expand_dims(X_train, axis=-1)\n",
    "X_test_reshaped = np.expand_dims(X_test, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf9682b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 20, 128)           66560     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2560)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               327808    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 394,497\n",
      "Trainable params: 394,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 20:06:01.952251: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-05 20:06:01.952320: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2024-11-05 20:06:01.952360: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2024-11-05 20:06:01.952395: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2024-11-05 20:06:01.982970: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2024-11-05 20:06:01.983130: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2024-11-05 20:06:01.983444: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "759/759 [==============================] - 12s 14ms/step - loss: 0.2668 - val_loss: 0.0447\n",
      "Epoch 2/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0481 - val_loss: 0.0455\n",
      "Epoch 3/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0458 - val_loss: 0.0442\n",
      "Epoch 4/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0445 - val_loss: 0.0431\n",
      "Epoch 5/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0441 - val_loss: 0.0434\n",
      "Epoch 6/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0438 - val_loss: 0.0431\n",
      "Epoch 7/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0439 - val_loss: 0.0431\n",
      "Epoch 8/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0438 - val_loss: 0.0429\n",
      "Epoch 9/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0438 - val_loss: 0.0431\n",
      "Epoch 10/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0438 - val_loss: 0.0430\n",
      "Epoch 11/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0437 - val_loss: 0.0429\n",
      "Epoch 12/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0437 - val_loss: 0.0430\n",
      "Epoch 13/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0436 - val_loss: 0.0429\n",
      "Epoch 14/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0436 - val_loss: 0.0429\n",
      "Epoch 15/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0436 - val_loss: 0.0430\n",
      "Epoch 16/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0436 - val_loss: 0.0430\n",
      "Epoch 17/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0436 - val_loss: 0.0429\n",
      "Epoch 18/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0437 - val_loss: 0.0431\n",
      "Epoch 19/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0437 - val_loss: 0.0429\n",
      "Epoch 20/20\n",
      "759/759 [==============================] - 10s 13ms/step - loss: 0.0437 - val_loss: 0.0431\n",
      "Mean Absolute Error: 0.043417696153482066\n",
      "Mean Absolute Percentage Error: 7.860186136036605\n",
      "Root Mean Square Error: 0.07916835719845637\n",
      "       Actual value  Predicted value\n",
      "0          0.432567         0.448031\n",
      "1          0.453756         0.448004\n",
      "2          0.309425         0.448187\n",
      "3          0.435730         0.448072\n",
      "4          0.447329         0.467005\n",
      "...             ...              ...\n",
      "13863      0.448684         0.448050\n",
      "13864      0.432491         0.447974\n",
      "13865      0.444417         0.448192\n",
      "13866      0.453932         0.448168\n",
      "13867      0.605493         0.447960\n",
      "\n",
      "[13868 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "batch_size = 64\n",
    "steps_per_epoch = len(X_train_reshaped) // batch_size\n",
    "\n",
    "# Define cyclic learning rate\n",
    "cyclic_lr = tfa.optimizers.CyclicalLearningRate(\n",
    "    initial_learning_rate=1e-04,\n",
    "    maximal_learning_rate=1e-02,\n",
    "    scale_fn=lambda x: 1 / (2 ** (x - 1)),\n",
    "    step_size=6 * steps_per_epoch\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = Adam(learning_rate=cyclic_lr, amsgrad=True)\n",
    "\n",
    "# Build the LSTM model\n",
    "def base_model_lstm():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=128, return_sequences=True, activation=\"relu\", \n",
    "                   input_shape=(X_train_reshaped.shape[1], 1), recurrent_dropout=0.2, \n",
    "                   kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(units=128, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(1))\n",
    "    return model\n",
    "\n",
    "# Initialize and compile the LSTM model\n",
    "lstm_model = base_model_lstm()\n",
    "lstm_model.compile(optimizer=optimizer, loss='mean_absolute_error')\n",
    "lstm_model.summary()\n",
    "\n",
    "# Further split training data into training and validation sets\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train_reshaped, y_train, test_size=0.125, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(\n",
    "    X_train_final, y_train_final, \n",
    "    validation_data=(X_val, y_val), \n",
    "    epochs=epoch,\n",
    "    batch_size=batch_size, \n",
    "    callbacks=[callback]\n",
    ")\n",
    "\n",
    "# Predict on test set\n",
    "y_predict = lstm_model.predict(X_test_reshaped).flatten()\n",
    "\n",
    "# Evaluation metrics\n",
    "meanAbErr = metrics.mean_absolute_error(y_test, y_predict)\n",
    "meanSqErr = metrics.mean_squared_error(y_test, y_predict)\n",
    "rootMeanSqErr = np.sqrt(meanSqErr)\n",
    "\n",
    "# Define MAPE function\n",
    "def MAPE(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "mape = MAPE(y_test, y_predict)\n",
    "\n",
    "# Print results\n",
    "print('Mean Absolute Error:', meanAbErr)\n",
    "print('Mean Absolute Percentage Error:', mape)\n",
    "print('Root Mean Square Error:', rootMeanSqErr)\n",
    "\n",
    "# Create a DataFrame for actual vs predicted values\n",
    "diff = pd.DataFrame({'Actual value': y_test.values, 'Predicted value': y_predict})\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a24398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
